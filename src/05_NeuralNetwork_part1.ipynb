{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "static-lafayette",
   "metadata": {},
   "source": [
    "Maxime Marchand\n",
    "# Astrophysics and Data Science : Project 5\n",
    "## Neural Network from scratch, part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-madness",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:49.586925Z",
     "start_time": "2023-11-29T19:24:48.746549Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-wealth",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "In this part, we load the dataset, comment its properties and prepare it to be used with the network. The linear normalization has been made using the following formula :\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y_i} = \\frac{2(y_i - y_{min})}{y_{max} - y_{min}} - 1\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\hat{y}_i$ is the normalized value of the element $y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-farming",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:49.600407Z",
     "start_time": "2023-11-29T19:24:49.590821Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading data file\n",
    "inFileName = 'Data/wheat.dat'\n",
    "data = np.loadtxt(inFileName, delimiter=',')\n",
    "\n",
    "# Randomize the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split the dataset\n",
    "vectors = data[:, :-1]\n",
    "classes = data[:, -1]\n",
    "\n",
    "# Normalize the dataset (see eq. above)\n",
    "for i in range(len(vectors[0])):\n",
    "    maxim = np.max(vectors[:, i])\n",
    "    minim = np.min(vectors[:, i])\n",
    "    vectors[:, i] = 2 * (vectors[:, i] - minim) / (maxim - minim) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-latin",
   "metadata": {},
   "source": [
    "Let make a corner plot to have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-robin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:56.696332Z",
     "start_time": "2023-11-29T19:24:50.188231Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of the quantities described in the dataset\n",
    "# (see https://machinelearningmastery.com/standard-machine-learning-datasets/)\n",
    "data_info = ['Area', 'Perimeter', 'Compactnes', 'Length of kernel', \n",
    "             'Width of kernel', 'Asymmetry coefficient', 'Length of kernel groove']\n",
    "\n",
    "# Boolean arrays to select the different classes\n",
    "cl1 = data[:, -1] == 1.0\n",
    "cl2 = data[:, -1] == 2.0\n",
    "cl3 = data[:, -1] == 3.0\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(vectors[0]), ncols=len(vectors[0]), figsize=(20, 15), dpi=300)\n",
    "fig.subplots_adjust(hspace=.25, wspace=.35)\n",
    "\n",
    "# Plotting the data\n",
    "for i in range(len(vectors[0])):\n",
    "    y = vectors[:, i]\n",
    "    for j in range(0, i):\n",
    "        x = vectors[:, j]\n",
    "        ax[i, j].scatter(x[cl1], y[cl1], c='b', alpha=0.3, label='class 1')\n",
    "        ax[i, j].scatter(x[cl2], y[cl2], c='r', alpha=0.3, label='class 2')\n",
    "        ax[i, j].scatter(x[cl3], y[cl3], c='g', alpha=0.3, label='class 3')\n",
    "        \n",
    "        ax[i, j].set_xlim([-1.1, 1.1])\n",
    "        ax[i, j].set_ylim([-1.1, 1.1])\n",
    "\n",
    "# Adding legend \n",
    "ax[1, 0].legend(bbox_to_anchor=(1, 1));\n",
    "\n",
    "# Adding axis labels\n",
    "for i in range(len(vectors[0])):\n",
    "    ax[len(vectors[0])-1, i].set_xlabel(data_info[i])\n",
    "    ax[i, 0].set_ylabel(data_info[i])\n",
    "\n",
    "# Removing upper triangle\n",
    "for i in range(len(vectors[0])):\n",
    "    for j in range(i, len(vectors[0])):\n",
    "        fig.delaxes(ax[i, j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-estimate",
   "metadata": {},
   "source": [
    "We now create a variable targets, where we convert the values of the classes variable using the following definition :\n",
    "\n",
    "\\begin{eqnarray}\n",
    "1 & \\longrightarrow & [1, 0, 0] \\\\\n",
    "2 & \\longrightarrow & [0, 1, 0] \\\\\n",
    "3 & \\longrightarrow & [0, 0, 1]\n",
    "\\end{eqnarray}\n",
    "\n",
    "We also split the dataset so that 80% of the data will be used for training, the rest for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-tracy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:56.713593Z",
     "start_time": "2023-11-29T19:24:56.705239Z"
    }
   },
   "outputs": [],
   "source": [
    "targets = np.zeros(shape=(len(classes), 3))    # List containing the targets [[1, 0, 0], [0, 1, 0], [0, 0, 1], ...]\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    cl = int(classes[i])\n",
    "    targets[i, cl-1] = 1.0\n",
    "\n",
    "training_percent = int(len(vectors) * 0.8)     # Percentage of the dataset that will be used for training (80 %)\n",
    "\n",
    "# We store training and validation data sets in two different dictionnaries\n",
    "training_set = {\n",
    "    'vectors' : vectors[:training_percent],\n",
    "    'classes' : classes[:training_percent],\n",
    "    'targets' : targets[:training_percent]\n",
    "}\n",
    "\n",
    "validation_set = {\n",
    "    'vectors' : vectors[training_percent:],\n",
    "    'classes' : classes[training_percent:],\n",
    "    'targets' : targets[training_percent:]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-plaintiff",
   "metadata": {},
   "source": [
    "Our data is ready. :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-monroe",
   "metadata": {},
   "source": [
    "### Creating the neural network\n",
    "We now implement a function that creates the neural network. It is implemented as a list of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-lancaster",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:56.732067Z",
     "start_time": "2023-11-29T19:24:56.716688Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_neural_network(num_neurons_input, num_hidden_layers, num_neurons_hidden, num_neurons_output):\n",
    "    \"\"\"\n",
    "    Creates the neural network\n",
    "    \n",
    "    PARAMETERS\n",
    "        num_neurons_input  : Number of neurons in the input layer\n",
    "        num_hidden_layers  : Number of hidden layers in the network\n",
    "        num_neurons_hidden : Number of neurons in the hidden layers (same number for each hidden layer)\n",
    "        num_neurons_output : Number of neurons in the output layer\n",
    "        \n",
    "    RETURNS\n",
    "        output : The network (implemented as a list of dictionnaries)\n",
    "    \"\"\"\n",
    "\n",
    "    output = []   # Variable to be returned\n",
    "    \n",
    "    # Creating the input layer ----------------------------------------------------------\n",
    "    input_layer = {}\n",
    "    \n",
    "    input_layer['I']  = np.hstack([np.zeros(num_neurons_input), np.ones(1)])\n",
    "    input_layer['W']  = np.random.normal(loc=0.0, scale=1/np.sqrt(len(input_layer['I'])/2), size=(num_neurons_input, len(input_layer['I']))) \n",
    "    input_layer['Y']  = np.zeros(shape=num_neurons_input)\n",
    "    input_layer['O']  = np.zeros(shape=num_neurons_input)\n",
    "    input_layer['dl'] = np.zeros(shape=num_neurons_input)\n",
    "    input_layer['DW'] = np.zeros(shape=input_layer['W'].shape)\n",
    "    \n",
    "    output.append(input_layer)\n",
    "    \n",
    "    # Creating the hidden layers --------------------------------------------------------\n",
    "    # Looping on each hidden layer. As we have already added the input layer, output[i] \n",
    "    # corresponds to the previous layer\n",
    "    for i in range(num_hidden_layers):\n",
    "        ilayer = {}\n",
    "        \n",
    "        ilayer['I']  = np.hstack([np.zeros(len(output[i]['O'])), np.ones(1)])\n",
    "        ilayer['W']  = np.random.normal(loc=0.0, scale=1/np.sqrt(len(ilayer['I'])/2), size=(num_neurons_hidden, len(ilayer['I'])))\n",
    "        ilayer['Y']  = np.zeros(shape=num_neurons_hidden)\n",
    "        ilayer['O']  = np.zeros(shape=num_neurons_hidden)\n",
    "        ilayer['dl'] = np.zeros(shape=num_neurons_hidden)\n",
    "        ilayer['DW'] = np.zeros(shape=ilayer['W'].shape)\n",
    "        \n",
    "        output.append(ilayer)\n",
    "\n",
    "    # Creating the output layer ----------------------------------------------------------\n",
    "    output_layer = {}\n",
    "\n",
    "    output_layer['I']  = np.hstack([np.zeros(len(output[-1]['O'])), np.ones(1)])\n",
    "    output_layer['W']  = np.random.normal(loc=0.0, scale=1/np.sqrt(len(output_layer['I'])/2), size=(num_neurons_output, len(output_layer['I'])))\n",
    "    output_layer['Y']  = np.zeros(shape=num_neurons_output)\n",
    "    output_layer['O']  = np.zeros(shape=num_neurons_output)\n",
    "    output_layer['dl'] = np.zeros(shape=num_neurons_output)\n",
    "    output_layer['DW'] = np.zeros(shape=output_layer['W'].shape)\n",
    "    \n",
    "    output.append(output_layer)\n",
    "    \n",
    "    # Checking the shapes ------------------------\n",
    "    # (Only for debugging purposes)\n",
    "    #for i in range(len(output)):\n",
    "    #    print(\"W_{0} : {1} | I_{0} : {2} | Y_{0} : {3} | O_{0} : {4} | DW_{0} : {5} | dl_{0} : {6}\".format(i, output[i]['W'].shape, output[i]['I'].shape, output[i]['Y'].shape, output[i]['O'].shape, output[i]['DW'].shape, output[i]['dl'].shape))\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-trinidad",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "We now implement all the functions that are used in order to do the forward proparagtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-sellers",
   "metadata": {},
   "source": [
    "##### Neuron activation\n",
    "The neuron activation $Y_l$ for the layer $l$ is given by : $Y_l = W_l \\cdot I_l$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-console",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:56.740794Z",
     "start_time": "2023-11-29T19:24:56.736035Z"
    }
   },
   "outputs": [],
   "source": [
    "def neuron_activation(network, layer):\n",
    "    return network[layer]['W'] @ network[layer]['I'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-covering",
   "metadata": {},
   "source": [
    "##### Softmax function (provided)\n",
    "The softmax function is given by :\n",
    "\\begin{equation}\n",
    "S(Y_{L-1}) = \\frac{1}{\\sum_{m=0}^{M-1} \\exp(Y_{L-1, m})} \\cdot \\begin{bmatrix} \\exp(Y_{L-1, 0}) \\\\ \\vdots \\\\ \\exp(Y_{L-1, M-1})  \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-rating",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:56.751452Z",
     "start_time": "2023-11-29T19:24:56.746013Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x, derivative=False):\n",
    "    exp_shifted = np.exp(x - x.max()) # for stability the values are shifted\n",
    "\n",
    "    if derivative:\n",
    "        return exp_shifted / np.sum(exp_shifted, axis=0) * (1 - exp_shifted / np.sum(exp_shifted, axis=0))\n",
    "    \n",
    "    else:\n",
    "        return exp_shifted / np.sum(exp_shifted, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-huntington",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:56.765792Z",
     "start_time": "2023-11-29T19:24:56.755054Z"
    }
   },
   "outputs": [],
   "source": [
    "softmax(np.array([1.25, 1.25]), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-commerce",
   "metadata": {},
   "source": [
    "##### Transfert functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-garbage",
   "metadata": {},
   "source": [
    "The sigmo√Ød function and its derivative are given by :\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\frac{1}{1+\\exp(-x)}\n",
    "\\hspace{3cm}\n",
    "f'(x) = \\frac{\\exp(-x)}{(1+\\exp(-x))^2}\n",
    "\\end{equation}\n",
    "\n",
    "The ReLU function and its derivative are given by :\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\left\\{ \\begin{array}{cc} 0 & x < 0 \\\\ x & x \\geq 0 \\end{array}  \\right. \n",
    "\\hspace{3cm}\n",
    "f'(x) = \\left\\{ \\begin{array}{cc} 0 & x < 0 \\\\ 1 & x \\geq 0 \\end{array}  \\right. \n",
    "\\end{equation}\n",
    "\n",
    "The ReLU dying function and its derivative are given by :\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\left\\{ \\begin{array}{rc} 0.01\\cdot x & x < 0 \\\\ x & x \\geq 0 \\end{array}  \\right. \n",
    "\\hspace{3cm}\n",
    "f'(x) = \\left\\{ \\begin{array}{rc} 0.01 & x < 0 \\\\ 1 & x \\geq 0 \\end{array}  \\right. \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-judgment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:56.774119Z",
     "start_time": "2023-11-29T19:24:56.768762Z"
    }
   },
   "outputs": [],
   "source": [
    "def transfert_sigmoid(network, layer, derivative=False):\n",
    "    \"\"\"\n",
    "    /!\\ This function should not be called directly\n",
    "    \"\"\"\n",
    "    Y = network[layer]['Y']\n",
    "    if not derivative:\n",
    "        return 1 / (1 + np.exp(-Y))\n",
    "    else:\n",
    "        return np.exp(-Y) / (1 + np.exp(-Y))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-climate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:56.785027Z",
     "start_time": "2023-11-29T19:24:56.778460Z"
    }
   },
   "outputs": [],
   "source": [
    "def transfert_ReLU(network, layer, derivative=False):\n",
    "    \"\"\"\n",
    "    /!\\ This function should not be called directly\n",
    "    \"\"\"\n",
    "    Y = network[layer]['Y']\n",
    "    lower = Y < 0.0    \n",
    "\n",
    "    if not derivative:\n",
    "        Y[lower] = 0.0\n",
    "        return Y\n",
    "    \n",
    "    else:\n",
    "        Y[lower] = 0.0\n",
    "        Y[~lower] = 1.0\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-monster",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:56.795525Z",
     "start_time": "2023-11-29T19:24:56.788192Z"
    }
   },
   "outputs": [],
   "source": [
    "def transfert_ReLU_dying(network, layer, derivative=False):\n",
    "    \"\"\"\n",
    "    /!\\ This function should not be called directly\n",
    "    \"\"\"\n",
    "    Y = network[layer]['Y']\n",
    "    lower = Y < 0.0\n",
    "    \n",
    "    if not derivative:\n",
    "        Y[lower] = 0.01 * Y[lower]\n",
    "        return Y\n",
    "\n",
    "    else:\n",
    "        Y[lower] = 0.01\n",
    "        Y[~lower] = 1.0\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-thunder",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:56.806719Z",
     "start_time": "2023-11-29T19:24:56.801791Z"
    }
   },
   "outputs": [],
   "source": [
    "def transfert(network, layer, derivative=False, mode='sig'):\n",
    "    if   mode == 'sig':\n",
    "        return transfert_sigmoid(network, layer, derivative)\n",
    "    elif mode == 'ReLU':\n",
    "        return transfert_ReLU(network, layer, derivative)\n",
    "    elif mode == 'ReLU_dying':\n",
    "        return transfert_ReLU_dying(network, layer, derivative)\n",
    "    else:\n",
    "        raise ValueError(\"{} is not a valid keyword. (choose between 'sig', 'ReLU' or 'ReLU_dying')\".format(mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-detective",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:56.815920Z",
     "start_time": "2023-11-29T19:24:56.809250Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_propagation(network, entry, transfert_mode):\n",
    "    # Input layer\n",
    "    network[0]['I'] = np.hstack([entry, np.ones(1)]) # Initializing first layer with data\n",
    "    network[0]['Y'] = neuron_activation(network, layer=0)\n",
    "    network[0]['O'] = transfert(network, layer=0, derivative=False, mode=transfert_mode)\n",
    "    \n",
    "    for l in range(1, len(network)-1):\n",
    "        network[l]['I'] = np.hstack([network[l-1]['O'], np.ones(1)])\n",
    "        network[l]['Y'] = neuron_activation(network, layer=l)\n",
    "        network[l]['O'] = transfert(network, layer=l, derivative=False, mode=transfert_mode)\n",
    "    \n",
    "    # Output layer\n",
    "    network[-1]['I'] = np.hstack([network[-2]['O'], np.ones(1)])\n",
    "    network[-1]['Y'] = neuron_activation(network, layer=-1)\n",
    "    network[-1]['O'] = softmax(network[-1]['Y'])\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-blade",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "We now implement the cost function and its gradient.\n",
    "\n",
    "The cost function $C : \\mathbb{R}^n \\longrightarrow \\mathbb{R}$ is given by the euclidean distance between the output vector $O = (O_0, \\dots, O_{n-1})\\in\\mathbb{R}^n$ and the target vector $t = (t_0, t_1, ..., t_{n-1}) \\in \\mathbb{R}^n$ :\n",
    "    \n",
    "\\begin{equation}\n",
    "    C = \\sqrt{\\sum_{i=0}^{n-1} (O_i - t_i)^2}\n",
    "\\end{equation}\n",
    "    \n",
    "The gradient $\\nabla C \\in \\mathbb{R}^n$ is thus given by :\n",
    "    \n",
    "\\begin{equation}\n",
    "    %\\nabla C = \\begin{bmatrix} 2\\cdot(O_0 - t_0) \\\\ \\vdots \\\\ 2\\cdot(O_{n-1} - t_{n-1}) \\end{bmatrix}\n",
    "    \\nabla C = \\frac{1}{\\sqrt{\\sum_{i=0}^{n-1} (O_i - t_i)^2}} \\cdot \\begin{bmatrix} (O_0 - t_0) \\\\ \\vdots \\\\ (O_{n-1} - t_{n-1}) \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-mechanism",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:56.869407Z",
     "start_time": "2023-11-29T19:24:56.866099Z"
    }
   },
   "outputs": [],
   "source": [
    "def cost(result, target):\n",
    "    return np.sqrt(np.sum((result - target)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-laugh",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:57.468984Z",
     "start_time": "2023-11-29T19:24:57.463914Z"
    }
   },
   "outputs": [],
   "source": [
    "def cost_gradient(result, target):\n",
    "    return (result - target) / cost(result, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-rabbit",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "We now implement all the functions that are used in order to do the backward propagation in our network.\n",
    "\n",
    "##### Back propagated error $\\delta_l$\n",
    "We now want to compute the back-propagated error $\\delta_l = \\frac{\\partial C}{\\partial Y_l}$ as a function of $\\delta_{l+1}$. Let first compute the back-propagated error of the last layer, $\\delta_{L-1} = \\frac{\\partial C}{\\partial Y_{L-1}}$ :\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial Y_{L-1}} = \n",
    "\\begin{bmatrix} \n",
    "    \\frac{\\partial C}{\\partial Y_{L-1, 0}} \\\\ \\vdots \\\\ \\frac{\\partial C}{\\partial Y_{L-1, n-1}} \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "    \\frac{\\partial C}{\\partial S_0(Y_{L-1})} \\frac{\\partial S_0(Y_{L-1})}{\\partial Y_{L-1}} \\\\ \\vdots \\\\\n",
    "    \\frac{\\partial C}{\\partial S_{n-1}(Y_{L-1})} \\frac{\\partial S_{n-1}(Y_{L-1})}{\\partial Y_{L-1}}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Where we used the definition of the transfert function of the output layer, which is the softmax function : $O(Y_{L-1}) = S(Y_{L-1})$. In order to compute the back propagated error for a layer $l$, it is easier to start from a concrete example. Let compute $\\delta_l$ as a function of $\\delta_{l+1}$, between two layers of size $3$ and $2$ as illustrated in the figure below.\n",
    "\n",
    "<img src='images/05_neuron.png' width=200>\n",
    "\n",
    "The activation of the layer $l+1$ is given by $Y_{l+1} = W_{l+1} \\cdot I_{l+1}$ with :\n",
    "\n",
    "\\begin{equation}\n",
    "W_{l+1} = \n",
    "\\begin{bmatrix} \n",
    "\\omega_{11} & \\omega_{12} & \\omega_{13} & {b_1} \\\\ \n",
    "\\omega_{21} & \\omega_{22} & \\omega_{23} & {b_2}\n",
    "\\end{bmatrix}\n",
    "\\quad ; \\quad\n",
    "I_{l+1} = \n",
    "\\begin{bmatrix}\n",
    "f_1(Y_{l, 1}) \\\\ \n",
    "f_2(Y_{l, 2}) \\\\ \n",
    "f_3(Y_{l, 3}) \\\\ \n",
    "1 \n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "We have $\\delta_{l}$ given by :\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta_l = \\frac{\\partial C}{\\partial Y_l} = \\frac{\\partial Y_{l+1}}{\\partial Y_l} \\frac{\\partial C}{\\partial Y_{l+1}} = \\frac{\\partial Y_{l+1}}{\\partial Y_l} \\cdot \\delta_{l+1}\n",
    "\\end{equation}\n",
    "\n",
    "In this example, $\\delta_{l+1} \\in \\mathbb{R}^{2}$, and thus $\\frac{\\partial Y_{l+1}}{\\partial Y_l} \\in Mat_{3, 2}(\\mathbb{R})$. Let now write $Y_{l+1}$ explicitely :\n",
    "\n",
    "\\begin{equation}\n",
    "Y_{l+1} =\n",
    "\\begin{bmatrix}\n",
    "Y_{l+1, 1} \\\\ Y_{l+1, 2}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\omega_{11} f_1(Y_{l, 1}) + \\omega_{12} f_2(Y_{l, 2}) + \\omega_{13} f_3(Y_{l, 3}) + {b_1} \\\\\n",
    "\\omega_{21} f_1(Y_{l, 1}) + \\omega_{22} f_2(Y_{l, 2}) + \\omega_{23} f_3(Y_{l, 3}) + {b_2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The partial derivatives are :\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial Y_{l+1, 1}}{\\partial Y_{l, 1}} = \\omega_{11} f_1'(Y_{l, 1}) & \\quad ; \\quad &\n",
    "\\frac{\\partial Y_{l+1, 2}}{\\partial Y_{l, 1}} = \\omega_{21} f_1'(Y_{l, 1}) \\\\\n",
    "\\frac{\\partial Y_{l+1, 1}}{\\partial Y_{l, 2}} = \\omega_{12} f_2'(Y_{l, 2}) & \\quad ; \\quad &\n",
    "\\frac{\\partial Y_{l+1, 2}}{\\partial Y_{l, 2}} = \\omega_{22} f_2'(Y_{l, 2}) \\\\\n",
    "\\frac{\\partial Y_{l+1, 1}}{\\partial Y_{l, 3}} = \\omega_{13} f_3'(Y_{l, 3}) & \\quad ; \\quad &\n",
    "\\frac{\\partial Y_{l+1, 2}}{\\partial Y_{l, 3}} = \\omega_{23} f_3'(Y_{l, 3})\n",
    "\\end{eqnarray}\n",
    "\n",
    "In matrix form, $\\dfrac{\\partial Y_{l+1}}{\\partial Y_l}$ is thus given by :\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial Y_{l+1}}{\\partial Y_l} = \n",
    "\\begin{bmatrix}\n",
    "\\omega_{11} f_1'(Y_{l, 1}) & \\omega_{21} f_1'(Y_{l, 1}) \\\\\n",
    "\\omega_{12} f_2'(Y_{l, 2}) & \\omega_{22} f_2'(Y_{l, 2}) \\\\\n",
    "\\omega_{13} f_3'(Y_{l, 3}) & \\omega_{23} f_3'(Y_{l, 3})\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "As one can observe, it corresponds to the weight matrix $W_{l+1}$ without the last column (the bias values) that has been transposed, and where each line has been multiplied by the derivative of the transfert function of the corresponding neuron in the layer $l$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-chase",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:58.392417Z",
     "start_time": "2023-11-29T19:24:58.382906Z"
    }
   },
   "outputs": [],
   "source": [
    "def backward_propagation(network, target, transfert_mode):\n",
    "    # Looping backward from the output layer to the input layer\n",
    "    for l in range(len(network)-1, -1, -1):\n",
    "        if l == len(network)-1: # output layer\n",
    "            gradC = cost_gradient(network[-1]['O'], target)\n",
    "            derivS = softmax(network[-1]['Y'], derivative=True)\n",
    "            network[-1]['dl'] = (gradC * derivS)\n",
    "\n",
    "        else:\n",
    "            derivatives = transfert(network, layer=l, derivative=True, mode=transfert_mode)\n",
    "            M = network[l+1]['W'][:, :-1].T.copy()   # Corresponds to the matrix dY_{l+1}/dY_{l}\n",
    "            \n",
    "            for i in range(len(derivatives)):\n",
    "                M[i, :] = M[i, :] * derivatives[i]\n",
    "            \n",
    "            network[l]['dl'] = M @ network[l+1]['dl']\n",
    "            \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-guarantee",
   "metadata": {},
   "source": [
    "##### Weights update\n",
    "\n",
    "Let now compute the matrix $\\Delta W_l = \\frac{\\partial C}{\\partial W_l}$ as a function of $\\delta_l$. Let $m$ and $n$ be respectively the number of neurons in the layers $l-1$ and $l$. Thus, $\\Delta W_{l}$ will be a matrix of $n$ lines and $m+1$ columns. The element $(\\Delta W)_{ij}$ is given by :\n",
    "\n",
    "\\begin{equation}\n",
    "\\Big(\\Delta W_l\\Big)_{ij} = \\frac{\\partial C}{\\partial W_{l, ij}} = \\frac{\\partial Y_{l,i}}{\\partial W_{l,ij}} \\frac{\\partial C}{\\partial Y_{l,i}} = \\frac{\\partial Y_{l,i}}{\\partial W_{l,ij}} \\delta_{l,i}\n",
    "\\end{equation}\n",
    "\n",
    "Let write $W_{l, ij} \\equiv \\omega_{ij}$. Thus :\n",
    "\n",
    "\\begin{equation}\n",
    "\\Big(\\Delta W_l\\Big)_{ij} = \\frac{\\partial}{\\partial \\omega_{ij}} \\left( \\sum_{k=0}^{M-1} \\omega_{lik} I_{lk} \\right)  \\delta_{l,i} = \\sum_{k=0}^{M-1} I_{lk} \\frac{\\partial\\omega_{lik}}{\\partial\\omega_{lij}} \\delta_{l, i} = I_{l, j} \\cdot \\delta_{l, i}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Where $\\frac{\\partial\\omega_{lik}}{\\partial\\omega_{lij}}$ is the Kronecker delta. The element $\\Delta W_{lij}$ is given by :\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta W_{lij} = I_{lj} \\cdot \\delta_{li} = \n",
    "\\begin{bmatrix}\n",
    "I_{l, 0} \\delta_{l, 0}    & I_{l, 1} \\delta_{l, 0} & \\cdots     & I_{l, m} \\delta_{l, 0}   \\\\\n",
    "I_{l, 0} \\delta_{l, 1}    & I_{l, 1} \\delta_{l, 1} & \\cdots     & I_{l, m} \\delta_{l, 1}   \\\\\n",
    "\\vdots                    & \\vdots                 & \\ddots     & \\vdots                     \\\\\n",
    "I_{l, 0} \\delta_{l, n-1}  & \\cdots                 & \\cdots     & I_{l, m} \\delta_{l, n-1}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-thong",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:24:59.371712Z",
     "start_time": "2023-11-29T19:24:59.366797Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_delta_weights(network):\n",
    "    for l in range(len(network)):\n",
    "        d = network[l]['dl']\n",
    "        I = network[l]['I']\n",
    "        network[l]['DW'] = (I[None, :] * d[:, None])\n",
    "        \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-evans",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-bermuda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:25:00.147143Z",
     "start_time": "2023-11-29T19:25:00.139907Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_network(network, data, target, transfert_mode):\n",
    "    \"\"\"\n",
    "    Initializes input layer with data, and update the network values\n",
    "    \"\"\"\n",
    "    network[0]['I'] = np.hstack([data, np.ones(1)])  # Initializing input layer with data\n",
    "    network = forward_propagation(network, data, transfert_mode)\n",
    "    network = backward_propagation(network, target, transfert_mode)\n",
    "    network = compute_delta_weights(network)\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-editor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:25:00.680794Z",
     "start_time": "2023-11-29T19:25:00.672893Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_network(network, num_epoch, learning_rate, training_set, transfert_mode):\n",
    "    \n",
    "    vec_cost = np.ndarray(num_epoch)  # Var. to store the evolution of the cost function\n",
    "    \n",
    "    for iepoch in tqdm(range(num_epoch)):\n",
    "        icost = 0\n",
    "        \n",
    "        for i in range(len(training_set['vectors'])):\n",
    "            network = update_network(network, training_set['vectors'][i], training_set['targets'][i], transfert_mode)\n",
    "            icost += cost(network[-1]['O'], training_set['targets'][i])\n",
    "            \n",
    "            # Updating the weights matrices\n",
    "            for l in range(len(network)):\n",
    "                network[l]['W'] = network[l]['W'] - learning_rate * network[l]['DW']\n",
    "            \n",
    "        vec_cost[iepoch] = icost/len(training_set['vectors'])\n",
    "\n",
    "            \n",
    "    return network, vec_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-housing",
   "metadata": {},
   "source": [
    "Wheat classification seem to work very fine with 1 hidden layer of 14 neurons, 100 epoch, learning rate 0.05 and sigmoid transfert function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-motion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:25:06.576174Z",
     "start_time": "2023-11-29T19:25:01.735805Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dim  = len(training_set['vectors'][0])          # Dimention of the data (7 in the case of wheat database)\n",
    "class_dim = len(np.unique(training_set['classes']))  # Number of possible outputs (3 in the case of wheat database)\n",
    "\n",
    "# Architecture of the network\n",
    "num_neurons_input  = data_dim\n",
    "num_hidden_layers  = 1\n",
    "num_neurons_hidden = 14\n",
    "num_neurons_output = class_dim\n",
    "\n",
    "# Training parameters\n",
    "num_epochs     = 100     # Good results with 100\n",
    "learning_rate  = 0.05    # Good results with 0.05\n",
    "transfert_mode = 'sig'   # Choose between 'sig', 'ReLU' and 'ReLU_dying'\n",
    "\n",
    "network = generate_neural_network(num_neurons_input, num_hidden_layers, num_neurons_hidden, num_neurons_output)\n",
    "network, vec_cost = train_network(network, num_epochs, learning_rate, training_set, transfert_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-index",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:25:07.879801Z",
     "start_time": "2023-11-29T19:25:07.680288Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Cost function\")\n",
    "ax.plot(vec_cost);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-palace",
   "metadata": {},
   "source": [
    "### Running on validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-guatemala",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:25:10.158442Z",
     "start_time": "2023-11-29T19:25:10.154448Z"
    }
   },
   "outputs": [],
   "source": [
    "def class_from_output(output):\n",
    "    \"\"\"\n",
    "    Converts target vector to class\n",
    "    \n",
    "    PARAMETERS\n",
    "        output : np.array\n",
    "        \n",
    "    RETURNS\n",
    "        1 if output == [1, 0, 0]\n",
    "        2 if output == [0, 1, 0]\n",
    "        3 if output == [0, 0, 1]\n",
    "    \"\"\"\n",
    "    return float(np.where(output == np.max(output))[0]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-narrow",
   "metadata": {},
   "source": [
    "## Confusion matrix and Accuracy\n",
    "The accuracy $A$ is computed with the confusion matrix $M$ by :\n",
    "\n",
    "\\begin{equation}\n",
    "A = \\frac{\\textrm{Tr}(M)}{\\sum_{i, j} M_{ij}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-recipient",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:25:11.080346Z",
     "start_time": "2023-11-29T19:25:11.062986Z"
    }
   },
   "outputs": [],
   "source": [
    "# We test the network with the validation data set\n",
    "predicted_values = np.ndarray(len(validation_set['vectors']))\n",
    "\n",
    "for i in range(len(validation_set['vectors'])):\n",
    "    entry      = validation_set['vectors'][i]                              # Data to put in network\n",
    "    network    = forward_propagation(network, entry, transfert_mode='sig') # Forward propagation \n",
    "    net_output = network[-1]['O']                                          # Predicted value from network\n",
    "    predicted_values[i] = class_from_output(net_output)                    # Storing the predicted class\n",
    "\n",
    "CM = np.zeros(shape=(class_dim, class_dim)) # Confusion Matrix\n",
    "\n",
    "for c in range(1, 4):     # c - class\n",
    "    for p in range(1, 4): # p - prediction\n",
    "        sel = (predicted_values == p) * (validation_set['classes'] == c)   # The * operator acts as the logic gate AND\n",
    "        CM[c-1, p-1] = len(sel[sel])\n",
    "\n",
    "print(\"Confusion matrix \\n\")\n",
    "print(\"{:<10} : {:>6} {:>6} {:>6}\".format(\"Predicted\", 1, 2, 3))\n",
    "print(\"Classes\")\n",
    "for i in range(len(CM)):\n",
    "    [print(\"{:^12} {:>6} {:>6} {:>6}\".format(i+1, *CM[i]))];\n",
    "    \n",
    "print(\"\\nAccuracy : {:.2f}%\".format(100*np.trace(CM)/np.sum(CM)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
